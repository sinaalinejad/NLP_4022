{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXmKyroYuybP"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoUu86p1Or1n"
      },
      "source": [
        "# Prediction-Based Word Vectors\n",
        "\n",
        "more recently prediction-based word vectors have demonstrated better performance, such as word2vec and GloVe (which also utilizes the benefit of counts). Here, we shall explore the embeddings produced by GloVe.\n",
        "\n",
        "Then run the following cells to load the GloVe vectors into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvpYg_7pODYJ",
        "outputId": "63b19a89-4d69-459a-dae1-ca47ec094660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "import pprint\n",
        "wv_from_bin = api.load(\"glove-wiki-gigaword-200\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFfCOLUsSSuS"
      },
      "source": [
        "### Words with Multiple Meanings\n",
        "Polysemes and homonyms are words that have more than one meaning (see this [wiki page](https://en.wikipedia.org/wiki/Polysemy) to learn more about the difference between polysemes and homonyms ). Find a word with *at least two different meanings* such that the top-10 most similar words (according to cosine similarity) contain related words from *both* meanings. For example, \"leaves\" has both \"go_away\" and \"a_structure_of_a_plant\" meaning in the top 10, and \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\". You will probably need to try several polysemous or homonymic words before you find one.\n",
        "\n",
        "Please state the word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous or homonymic words you tried didn't work (i.e. the top-10 most similar words only contain **one** of the meanings of the words)?\n",
        "\n",
        "**Note**: You should use the `wv_from_bin.most_similar(word)` function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance, please check the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAr09U-xSSuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdca37be-dbff-4d07-b65d-7c62e75655ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bats', 0.691724419593811),\n",
              " ('batting', 0.6160588264465332),\n",
              " ('balls', 0.5692734122276306),\n",
              " ('batted', 0.5530908107757568),\n",
              " ('toss', 0.5506128668785095),\n",
              " ('wicket', 0.5495278835296631),\n",
              " ('pitch', 0.5489361882209778),\n",
              " ('bowled', 0.5452010631561279),\n",
              " ('hitter', 0.5353438854217529),\n",
              " ('batsman', 0.5348091125488281)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "wv_from_bin.most_similar(\"bat\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv_from_bin.most_similar(\"kind\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVWaWM8lvdpL",
        "outputId": "0f26e75c-6cd9-44f6-a3ab-1dcca55323a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sort', 0.9320126175880432),\n",
              " ('something', 0.8606709241867065),\n",
              " ('thing', 0.8369148969650269),\n",
              " ('really', 0.8218013644218445),\n",
              " ('what', 0.79334956407547),\n",
              " ('nothing', 0.791183590888977),\n",
              " ('think', 0.7800678610801697),\n",
              " ('anything', 0.7773789167404175),\n",
              " ('you', 0.7736413478851318),\n",
              " ('seems', 0.7729967832565308)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv_from_bin.most_similar(\"right\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwYBWR_Qwh_Y",
        "outputId": "0a283de9-f692-4a0e-8145-05385a960086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('left', 0.716508150100708),\n",
              " ('if', 0.6925000548362732),\n",
              " (\"n't\", 0.6774845719337463),\n",
              " ('back', 0.6770386099815369),\n",
              " ('just', 0.6740819811820984),\n",
              " ('but', 0.667771577835083),\n",
              " ('out', 0.6671877503395081),\n",
              " ('put', 0.665894091129303),\n",
              " ('hand', 0.6634083390235901),\n",
              " ('want', 0.6615420579910278)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv_from_bin.most_similar(\"bank\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5EzfvN2ws2V",
        "outputId": "5855a48a-d05f-4857-8695-d4168b974120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('banks', 0.7625691294670105),\n",
              " ('banking', 0.6818838119506836),\n",
              " ('central', 0.6283639073371887),\n",
              " ('financial', 0.6166563034057617),\n",
              " ('credit', 0.6049750447273254),\n",
              " ('lending', 0.5980608463287354),\n",
              " ('monetary', 0.5963003039360046),\n",
              " ('bankers', 0.5913101434707642),\n",
              " ('loans', 0.5802939534187317),\n",
              " ('investment', 0.5740203261375427)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdQ018tjSSuT"
      },
      "source": [
        "### SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv_from_bin.most_similar(\"lie\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiPE1fHxwxdE",
        "outputId": "f91c689e-e6a1-48c3-fd2c-571044dc3c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('lying', 0.6884376406669617),\n",
              " ('lies', 0.6648906469345093),\n",
              " ('lay', 0.49874207377433777),\n",
              " ('beneath', 0.4979451894760132),\n",
              " ('hide', 0.4929002821445465),\n",
              " ('exist', 0.49129071831703186),\n",
              " ('sit', 0.48945313692092896),\n",
              " ('truth', 0.4867425262928009),\n",
              " ('bare', 0.48414283990859985),\n",
              " ('these', 0.48072928190231323)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfeW-eK9SSuU"
      },
      "source": [
        "### Synonyms & Antonyms\n",
        "\n",
        "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
        "\n",
        "Find three words $(w_1,w_2,w_3)$ where $w_1$ and $w_2$ are synonyms and $w_1$ and $w_3$ are antonyms, but Cosine Distance $(w_1,w_3) <$ Cosine Distance $(w_1,w_2)$.\n",
        "\n",
        "As an example, $w_1$=\"happy\" is closer to $w_3$=\"sad\" than to $w_2$=\"cheerful\". Please find a different example that satisfies the above. Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the `wv_from_bin.distance(w1, w2)` function here in order to compute the cosine distance between two words. Please see the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.distance)__ for further assistance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwlpPjpHSSuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3123f606-cad1-4add-be33-27154052b9a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyms fast, quick have cosine distance: 0.3328641653060913\n",
            "Antonyms fast, slow have cosine distance: 0.2522680163383484\n"
          ]
        }
      ],
      "source": [
        "w1 = \"fast\"   # \"male\"   # \"good\"\n",
        "w2 = \"quick\"  # \"man\"    # \"nice\"\n",
        "w3 = \"slow\"   # \"female\" # \"bad\"\n",
        "w1_w2_dist = wv_from_bin.distance(w1, w2)\n",
        "w1_w3_dist = wv_from_bin.distance(w1, w3)\n",
        "\n",
        "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w1_w2_dist))\n",
        "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w1_w3_dist))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeIHjTFMSSuV"
      },
      "source": [
        "### SOLUTION: because for example fast and slow have happened in same context more than fast and quick. it is the matter of how much the two words have been used in same context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxIDq26zSSuW"
      },
      "source": [
        "### Analogies with Word Vectors\n",
        "Word vectors have been shown to *sometimes* exhibit the ability to solve analogies.\n",
        "\n",
        "As an example, for the analogy \"man : grandfather :: woman : x\" (read: man is to grandfather as woman is to x), what is x?\n",
        "\n",
        "In the cell below, we show you how to use word vectors to find x using the `most_similar` function from the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar)__. The function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list. The answer to the analogy will have the highest cosine similarity (largest returned numerical value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0pC7H4VSSuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb3352cb-5910-4269-c751-5486909e302b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('grandmother', 0.7608445286750793),\n",
            " ('granddaughter', 0.7200808525085449),\n",
            " ('daughter', 0.7168302536010742),\n",
            " ('mother', 0.7151536345481873),\n",
            " ('niece', 0.7005682587623596),\n",
            " ('father', 0.6659887433052063),\n",
            " ('aunt', 0.6623408794403076),\n",
            " ('grandson', 0.6618767976760864),\n",
            " ('grandparents', 0.644661009311676),\n",
            " ('wife', 0.6445354223251343)]\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to answer the analogy -- man : grandfather :: woman : x\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'grandfather'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVv8I9WwSSuZ"
      },
      "source": [
        "Let $m$, $g$, $w$, and $x$ denote the word vectors for `man`, `grandfather`, `woman`, and the answer, respectively. Using **only** vectors $m$, $g$, $w$, and the vector arithmetic operators $+$ and $-$ in your answer, to what expression are we maximizing $x$'s cosine similarity?\n",
        "\n",
        "Hint: Recall that word vectors are simply multi-dimensional vectors that represent a word. It might help to draw out a 2D example using arbitrary locations of each vector. Where would `man` and `woman` lie in the coordinate plane relative to `grandfather` and the answer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlUKBqtHSSuZ"
      },
      "source": [
        "### SOLUTION: g + w - m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rRgMca9SSua"
      },
      "source": [
        "### Finding Analogies\n",
        "a. For the previous example, it's clear that \"grandmother\" completes the analogy. But give an intuitive explanation as to why the `most_similar` function gives us words like \"granddaughter\", \"daughter\", or \"mother?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgYQXazQSSua"
      },
      "source": [
        "### SOLUTION: it gives these words because these words are somehow near the words woman or grandfather and far from the word man."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9aAUXEISSub"
      },
      "source": [
        "b. Find an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy is complicated, explain why the analogy holds in one or two sentences.\n",
        "\n",
        "**Note**: You may have to try many analogies to find one that works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhzQJMYYVSjf"
      },
      "outputs": [],
      "source": [
        "x, y, a, b = \"iran\", \"japan\", \"tehran\", \"tokyo\"\n",
        "assert wv_from_bin.most_similar(positive=[a, y], negative=[x])[0][0] == b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3QlPqAwSSub"
      },
      "source": [
        "### SOLUTION: iran:tehran :: japan:tokyo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwgcEywwSSuc"
      },
      "source": [
        "### Incorrect Analogy\n",
        "a. Below, we expect to see the intended analogy \"hand : glove :: foot : **sock**\", but we see an unexpected result instead. Give a potential reason as to why this particular analogy turned out the way it did?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-ykWoJoSSuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989e0153-44a1-4f1d-fc04-8656e3cdb3c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('45,000-square', 0.4922032654285431),\n",
            " ('15,000-square', 0.4649604558944702),\n",
            " ('10,000-square', 0.4544755816459656),\n",
            " ('6,000-square', 0.44975775480270386),\n",
            " ('3,500-square', 0.444133460521698),\n",
            " ('700-square', 0.44257497787475586),\n",
            " ('50,000-square', 0.4356396794319153),\n",
            " ('3,000-square', 0.43486514687538147),\n",
            " ('30,000-square', 0.4330596923828125),\n",
            " ('footed', 0.43236875534057617)]\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(wv_from_bin.most_similar(positive=['foot', 'glove'], negative=['hand']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn4ruS8MSSud"
      },
      "source": [
        "### SOLUTION: because the word 'foot' is a unit for measuring distance too. and the goal is to find words near to this word and far from hand, so it is outputing measurement units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1gHyZt0SSud"
      },
      "source": [
        "b. Find another example of analogy that does *not* hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the **incorrect** value of b according to the word vectors (in the previous example, this would be **'45,000-square'**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_rlci42XQTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce578125-a62e-4f2a-e27e-623cc7c47356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('tonight', 0.6419892907142639),\n",
            " ('sure', 0.6270460486412048),\n",
            " ('really', 0.6245638728141785),\n",
            " ('terrific', 0.6219825148582458),\n",
            " ('pretty', 0.6166312098503113),\n",
            " (\"n't\", 0.609019935131073),\n",
            " ('excellent', 0.6084809899330139),\n",
            " ('always', 0.6062433123588562),\n",
            " ('something', 0.6053240299224854),\n",
            " (\"'re\", 0.6035588383674622)]\n"
          ]
        }
      ],
      "source": [
        "x, y, a, b = \"day\", \"good\", \"night\", \"bad\"\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[a, y], negative=[x]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4x0EHjeSSue"
      },
      "source": [
        "### SOLUTION:\n",
        "day:night :: good:bad, tonight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvlycXN-SSuf"
      },
      "source": [
        "### Guided Analysis of Bias in Word Vectors\n",
        "\n",
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n",
        "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"profession\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"profession\" and most dissimilar to \"woman\". Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XggWA4MhSSuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9a8a4c-a247-45d8-9e0a-0f7287344fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('reputation', 0.5250176787376404),\n",
            " ('professions', 0.5178037881851196),\n",
            " ('skill', 0.49046966433525085),\n",
            " ('skills', 0.49005505442619324),\n",
            " ('ethic', 0.4897659420967102),\n",
            " ('business', 0.4875852167606354),\n",
            " ('respected', 0.485920250415802),\n",
            " ('practice', 0.482104629278183),\n",
            " ('regarded', 0.4778572618961334),\n",
            " ('life', 0.4760662019252777)]\n",
            "\n",
            "[('professions', 0.5957457423210144),\n",
            " ('practitioner', 0.49884122610092163),\n",
            " ('teaching', 0.48292139172554016),\n",
            " ('nursing', 0.48211804032325745),\n",
            " ('vocation', 0.4788965880870819),\n",
            " ('teacher', 0.47160351276397705),\n",
            " ('practicing', 0.46937814354896545),\n",
            " ('educator', 0.46524327993392944),\n",
            " ('physicians', 0.4628995358943939),\n",
            " ('professionals', 0.4601394236087799)]\n"
          ]
        }
      ],
      "source": [
        "# Run this cell\n",
        "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be most dissimilar from.\n",
        "\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'profession'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4g6KbsYSSuh"
      },
      "source": [
        "### SOLUTION:\n",
        "there is a gender bias here. the jobs like nursing and teacher is considered to be for women but we can not see these words in man related ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxJmnS6lSSui"
      },
      "source": [
        "### Independent Analysis of Bias in Word Vectors\n",
        "\n",
        "Use the `most_similar` function to find another pair of analogies that demonstrates some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZoDheIfSSui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e363b8a5-07a7-4b1e-85b2-e070aa951a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('lovely', 0.6010177135467529),\n",
            " ('beautiful', 0.5075923800468445),\n",
            " ('quiet', 0.5043780207633972),\n",
            " ('bright', 0.4798499345779419),\n",
            " ('bouquet', 0.4636004567146301),\n",
            " ('happy', 0.4616834223270416),\n",
            " ('wonderful', 0.4579712152481079),\n",
            " ('festive', 0.4563849866390228),\n",
            " ('nice', 0.44969430565834045),\n",
            " ('sweet', 0.4492822289466858)]\n",
            "\n",
            "[('enjoyable', 0.4733288586139679),\n",
            " ('unpleasant', 0.4511060416698456),\n",
            " ('termites', 0.4320865571498871),\n",
            " ('pleasurable', 0.4303135573863983),\n",
            " ('surroundings', 0.42313987016677856),\n",
            " ('rodents', 0.4011191725730896),\n",
            " ('mosquitoes', 0.3891984522342682),\n",
            " ('pests', 0.38524946570396423),\n",
            " ('arthropods', 0.38287049531936646),\n",
            " ('harmless', 0.3790086805820465)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "A = \"flowers\"\n",
        "B = \"insects\"\n",
        "word = \"pleasant\"\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[A, word], negative=[B]))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[B, word], negative=[A]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGOlmtJoSSuj"
      },
      "source": [
        "### SOLUTION:\n",
        "Word vectors can capture societal stereotypes and associations. For instance, the association between “flowers” and “pleasant” versus “insects” and “unpleasant” reflects cultural biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK2XVWzmSSuk"
      },
      "source": [
        "### Thinking About Bias\n",
        "\n",
        "a. Give one explanation of how bias gets into the word vectors. Briefly describe a real-world example that demonstrates this source of bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19pM85fCSSuk"
      },
      "source": [
        "### SOLUTION:\n",
        "Bias can enter word vectors primarily through the data they are trained on. If the training data contains societal biases, these can be reflected in the word vectors. For example, if a corpus has more sentences associating men with technology and women with domestic roles, the resulting word vectors will likely mirror these biases.\n",
        "\n",
        "A real-world example of this is seen in a study where word embeddings were found to associate male names more closely with career-oriented words and female names with family-oriented words, reflecting gender stereotypes present in the training data. This type of bias can perpetuate stereotypes and affect decisions made by AI systems using these word vectors, such as resume filtering in hiring processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILYqJZ7ASSul"
      },
      "source": [
        "b. What is one method you can use to mitigate bias exhibited by word vectors?  Briefly describe a real-world example that demonstrates this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnJaAB7mSSul"
      },
      "source": [
        "### SOLUTION:\n",
        "One method to mitigate bias in word vectors is through debiasing techniques such as Hard Debiasing. This approach involves identifying bias directions in the word vector space and then neutralizing and equalizing vectors to reduce gender bias.\n",
        "\n",
        "A real-world example of this method in action is the modification of the Word2Vec model to reduce gender bias. Researchers identified gender stereotypes in the model and applied debiasing techniques to adjust the vectors, resulting in a less biased representation of words related to gender. This helps in creating more fair and equitable NLP applications, such as job recommendation systems that are less likely to perpetuate gender biases."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}