{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4NQTign_k_T"
      },
      "source": [
        "#Q1: Probabilistic N-Gram Language Model(50 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDKtnG-HAH1k"
      },
      "source": [
        "**Objective:**\n",
        "\n",
        "The objective of this question is to implement and experiment with an N-Gram language model using the Reuters dataset. The task involves building a probabilistic N-Gram model and creating a text generator based on the trained model with customizable parameters.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "\n",
        "**1.Text Preprocessing (5 points):**\n",
        "*   Implement the preprocess_text function to perform necessary text preprocessing. You may use NLTK or other relevant libraries for this task. (Already provided, no modification needed)\n",
        "\n",
        "\n",
        "**2.Build Probabilistic N-Gram Model (15 points):**\n",
        "\n",
        "*   Implement the build_probabilistic_ngram_model function to construct a probabilistic N-Gram model from the Reuters dataset.\n",
        "\n",
        "\n",
        "**3.Generate Text with Customizable Parameters (15 points):**\n",
        "\n",
        "*   Implement the generate_text function to generate text given a seed text and the probabilistic N-Gram model.\n",
        "*   The function should have parameters for probability_threshold and min_length to customize the generation process.\n",
        "*   Ensure that the generation stops when either the specified min_length is reached or the probabilities fall below probability_threshold.\n",
        "\n",
        "\n",
        "**4.Experimentation and Parameter Tuning (5 points):**\n",
        "\n",
        "*   Use Google Colab to experiment with different values of n_value, probability_threshold, and min_length.\n",
        "Find the optimal parameters that result in coherent and meaningful generated text.\n",
        "*   Provide a detailed analysis of the impact of changing each parameter on the generated text's quality.\n",
        "*   Discuss any challenges faced during parameter tuning and propose potential improvements.\n",
        "\n",
        "\n",
        "**5.Results and Conclusion (10 points):**\n",
        "\n",
        "*   Summarize your findings and present the optimal parameter values for n_value, probability_threshold, and min_length.\n",
        "*   Discuss the trade-offs and considerations when selecting these parameters.\n",
        "*   Conclude with insights gained from the experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NWXJy-T-Vd7",
        "outputId": "c7070996-091f-4055-fe94-3a238a912282"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "from nltk import ngrams\n",
        "import random\n",
        "import string\n",
        "from collections import defaultdict\n",
        "\n",
        "# Download the Reuters dataset if not already downloaded\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rlUHSWrPPhSm",
        "outputId": "f1149e70-1cf6-4993-ea02-61b7c97cdfa4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ltdA90jZ0oJD"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7zGo5FJJZv2g"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def split_sentences(text):\n",
        "    sentences = re.split(r\"<eos>\", text)\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Q9IHxAbU0N80"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Fill in: Implement text preprocessing steps like lowercasing, removing punctuation, etc.\n",
        "    # You may use NLTK or other libraries for this.\n",
        "\n",
        "    # Step 1: Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Step 2: Sentence tokenizing\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Step 3: Removing punctuations\n",
        "    new_sentences = []\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    for sentence in sentences:\n",
        "      new_sentence = sentence.translate(translator)\n",
        "      new_sentence = new_sentence + \" <eos>\"\n",
        "      new_sentences.append(new_sentence)\n",
        "\n",
        "    return \" \".join(new_sentences)\n",
        "\n",
        "# Function to build a probabilistic n-gram model\n",
        "def build_probabilistic_ngram_model(corpus, n):\n",
        "    # Fill in: Implement code to build an n-gram model from the given corpus.\n",
        "    # You may use NLTK's word_tokenize function.\n",
        "    model = {}\n",
        "    vocab = [\"<eos>\"]\n",
        "    for text in corpus:\n",
        "      sentences = split_sentences(text)\n",
        "      for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        for token in tokens:\n",
        "          if token not in vocab:\n",
        "            vocab.append(token)\n",
        "        tokens.append(\"<eos>\")\n",
        "        tokens = [\"<bos>\"] + tokens\n",
        "        ngram_list = []\n",
        "        for i in range(n):\n",
        "          ngram_list.extend(list(ngrams(tokens,n-i)))\n",
        "        # ngram_list = [ngram_list.extend(list(ngrams(tokens,n-i))) for i in range(n)]\n",
        "        # ngram_list = list(ngrams(tokens, n)) + list(ngrams(tokens, n-1))\n",
        "        for ngram in ngram_list:\n",
        "          if ngram not in model:\n",
        "            model[ngram] = 0\n",
        "          model[ngram] = model[ngram] + 1\n",
        "    return model, vocab\n",
        "\n",
        "def generate_next_word(key, model, vocab, probability_threshold):\n",
        "  if len(key) == 0: # This means we have reached unigrams\n",
        "    # checking unigrams\n",
        "    cnt = sum([model[(token,)] for token in vocab])\n",
        "    prob_tokens = {}\n",
        "    for token in vocab:\n",
        "      new_key = (token,)\n",
        "      new_cnt = model[new_key]\n",
        "      prob_tokens[token] = new_cnt / cnt\n",
        "  else:\n",
        "    cnt = model.get(tuple(key)) # number of occurence of the words which are known to us (key)\n",
        "    prob_tokens = {} # To save the probs of words to be the next word\n",
        "    under_tresh = False # To check if max prob is under the threshold\n",
        "    zero_prob = False # To indicate if there is no such combination of words in the training dataset, in that case we use a lower-order of grams (n-1,n-2,...,1)\n",
        "    if cnt == None: # if the key itself is did not occur in train set, set the zero_prob\n",
        "      zero_prob = True\n",
        "    else:\n",
        "      for token in vocab:\n",
        "        new_key = key + [token,] # The new key based on this token\n",
        "        new_cnt = model.get(tuple(new_key)) # number of occurences of this token provided that the n-1 previous words are before it.\n",
        "        if new_cnt == None: # If we don't have this token after n-1 previous words in train set, set the prob of it to zero\n",
        "          prob_tokens[token] = 0\n",
        "        else:\n",
        "          prob_tokens[token] = new_cnt / cnt # this will be the prob of this token to come next regarding to n-1 previous words\n",
        "      max_prob = max([value for value in prob_tokens.values()]) # Calculate the maximum value of probs for tokens\n",
        "      if max_prob < probability_threshold: # if max prob is less than the thresh, we set under_thresh to maybe stop the generation\n",
        "        under_tresh = True\n",
        "        if max_prob == 0: # if the max prob is zero, means all probs are zero, in that case we use a lower-order of grams (n-1,n-2,...,1)\n",
        "          zero_prob = True\n",
        "  next_word = \"\"\n",
        "  if not zero_prob:\n",
        "    next_word = random.choices(list(prob_tokens.keys()), weights=prob_tokens.values(), k=1)[0] # predicting next word using roullete-wheel. probs are weights and greater weight means greater chance to be picked\n",
        "  return next_word, under_tresh, zero_prob\n",
        "\n",
        "\n",
        "# Function to generate text using the probabilistic n-gram model with stop criteria\n",
        "def generate_text(model, vocab, seed_text, n, probability_threshold=0.1, min_length=10):\n",
        "    # Fill in: Implement code to generate text given a seed text and the n-gram model.\n",
        "    # Use the model to predict the next words and generate a sequence.\n",
        "    generated_text = seed_text\n",
        "    # Preprocessing the seed text\n",
        "    seed_text = seed_text.lower()\n",
        "    # Tokenizing the seed text\n",
        "    seed_tokens = word_tokenize(seed_text)\n",
        "    tokens_cnt = len(seed_tokens)\n",
        "    # Add extra <bos> to first of seed text if needed\n",
        "    seed_tokens = [\"<bos>\"] + seed_tokens\n",
        "    seed_tokens = [\"<bos>\"] * (n - len(seed_tokens) - 1) + seed_tokens\n",
        "    # Where to start the key based on the value of n in n-gram\n",
        "    start_index = len(seed_tokens) - n + 1\n",
        "    # The key\n",
        "    key = seed_tokens[start_index:]\n",
        "    min_length_reached = False\n",
        "    while(True):\n",
        "      next_word, under_thresh, zero_prob = generate_next_word(key, model, vocab, probability_threshold)\n",
        "      if zero_prob: # In this case, try (n-1)-gram, (n-2)-gram, ..., unigram\n",
        "        start_index += 1\n",
        "        key = seed_tokens[start_index:] # The new key for (i-1)-gram\n",
        "        continue\n",
        "      if (under_thresh and min_length_reached) or (next_word==\"<eos>\" and min_length_reached): # Applying stopping criteria (max prob under prob_threshold, min_length reached, or next word being <eos>)\n",
        "        break\n",
        "      # Updating number of tokens to check if we reached min_length\n",
        "      tokens_cnt += 1\n",
        "      if tokens_cnt == min_length:\n",
        "        min_length_reached = True\n",
        "      # Adding next word which is generated to generated_text\n",
        "      generated_text = generated_text + \" \" + next_word\n",
        "      seed_tokens.append(next_word)\n",
        "      # Updaing start_index\n",
        "      start_index = len(seed_tokens) - n + 1\n",
        "      # The new key for next step and prediction of next word\n",
        "      key = seed_tokens[start_index:]\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eVVMe_s59Ngd"
      },
      "outputs": [],
      "source": [
        "# Load the Reuters dataset\n",
        "corpus = [reuters.raw(file_id) for file_id in reuters.fileids()]\n",
        "\n",
        "# Preprocess the entire corpus\n",
        "preprocessed_corpus = [preprocess_text(text) for text in corpus]\n",
        "\n",
        "# Choose an n for the n-gram model\n",
        "n_value = 5  # You may change this value\n",
        "\n",
        "# Build the probabilistic n-gram model\n",
        "probabilistic_ngram_model, vocab = build_probabilistic_ngram_model(preprocessed_corpus, n_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWBuuQYw2tYR",
        "outputId": "a72ec154-833e-4cc2-efa8-8577b8f9f920"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Inflation is forecast initially apply after a spokeswoman was the change in 1986 and quotas which would range\n"
          ]
        }
      ],
      "source": [
        "# n_value=2, probability_threshold=0.01, min_length=10\n",
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, vocab, seed_text, n_value, probability_threshold=0.01, min_length=10)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnAxF-BQ2uG-",
        "outputId": "77e924aa-606e-4bd9-ad69-6c085093404a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Inflation is fraught with 195000 dlrs of the buyout or\n"
          ]
        }
      ],
      "source": [
        "# n_value=2, probability_threshold=0.05, min_length=10\n",
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, vocab, seed_text, n_value, probability_threshold=0.05, min_length=10)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kYSNWM_2uiq",
        "outputId": "19c5febf-94d2-47d5-b2dc-a4deea0a56dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Inflation is expected economic advisors convinced of about 63 cts payable may 31 net 23 mln\n"
          ]
        }
      ],
      "source": [
        "# n_value=2, probability_threshold=0.1, min_length=10\n",
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, vocab, seed_text, n_value, probability_threshold=0.1, min_length=10)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kCJRiy8yzr_",
        "outputId": "0572c265-a528-4857-95ee-7c554de54194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Inflation is likely to tell you when youve hit someone\n"
          ]
        }
      ],
      "source": [
        "# n_value=3, probability_threshold=0.05, min_length=10\n",
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, vocab, seed_text, n_value, probability_threshold=0.05, min_length=10)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K57s7M800Urp",
        "outputId": "dc4d5d6e-c224-4690-9193-bc0392011764"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Inflation is rising with trade unions actu had threatened to take economic and political issues he said\n"
          ]
        }
      ],
      "source": [
        "# n_value=3, probability_threshold=0.01, min_length=10\n",
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, vocab, seed_text, n_value, probability_threshold=0.01, min_length=10)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgjcUkFU0noi",
        "outputId": "3e463671-7a7e-4a48-a4ec-0abd2ea23540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Inflation is expected to be exchanged for 406 shares of\n"
          ]
        }
      ],
      "source": [
        "# n_value=3, probability_threshold=0.1, min_length=10\n",
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, vocab, seed_text, n_value, probability_threshold=0.1, min_length=10)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tcuvITqF8Si",
        "outputId": "b361179f-4a08-4bea-d59f-21dfc628ede6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Inflation is running at an annual rate of 347 mln units the national association of realtors nar said\n"
          ]
        }
      ],
      "source": [
        "# n_value=4, probability_threshold=0.01, min_length=10\n",
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, vocab, seed_text, n_value, probability_threshold=0.01, min_length=10)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbCwtW-l1F2C",
        "outputId": "d7c8eeec-c663-4b4b-f149-ab2e29e0a010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Inflation is expected to remain above 40 billion dlrs is about the same as for common stock\n"
          ]
        }
      ],
      "source": [
        "# n_value=4, probability_threshold=0.05, min_length=10\n",
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, vocab, seed_text, n_value, probability_threshold=0.05, min_length=10)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quM2ouwq1GbT",
        "outputId": "2f819a70-b7ef-4a9e-9df5-41b4e36f39df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Inflation is not such a constructive factor as this time last year\n"
          ]
        }
      ],
      "source": [
        "# n_value=4, probability_threshold=0.1, min_length=10\n",
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, vocab, seed_text, n_value, probability_threshold=0.1, min_length=10)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM8dz1_eTa6T",
        "outputId": "b3febb5c-49cc-4e6b-a606-c25c752bc6e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Inflation is not such a constructive factor as this time last year and 68 billion on october 5 he declined to elaborate\n"
          ]
        }
      ],
      "source": [
        "# n_value=5, probability_threshold=0.01, min_length=10\n",
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, vocab, seed_text, n_value, probability_threshold=0.01, min_length=10)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW1WvGjKTbmG",
        "outputId": "de04f3ac-48ee-4ca6-ad50-44b276447c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Inflation is running at an annual rate of 25 pct or slightly above is not convinced that growth will pick up in future\n"
          ]
        }
      ],
      "source": [
        "# n_value=5, probability_threshold=0.05, min_length=10\n",
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, vocab, seed_text, n_value, probability_threshold=0.05, min_length=10)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80ul1RCjTcDz",
        "outputId": "b42f575b-b387-4e55-c0dc-226f79a68390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Inflation is expected to be completed within 60 days at which time the mcdowell board would be restructured to include interpharm management\n"
          ]
        }
      ],
      "source": [
        "# n_value=5, probability_threshold=0.1, min_length=10\n",
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, vocab, seed_text, n_value, probability_threshold=0.1, min_length=10)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ3XzDx7JUNN"
      },
      "source": [
        "#Q2: Sentiment Analysis with Naive Bayes Classifier(50 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMuVkjW2XfAp"
      },
      "source": [
        "**Objective:**\n",
        "\n",
        "You are tasked with implementing a Naive Bayes classifier for sentiment analysis. The provided code is incomplete, and your goal is to complete the missing parts. Additionally, you should train the classifier on a small dataset and analyze its performance.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1.**Complete the Code (35 points)**: Fill in the missing parts in the provided Python code for the Naive Bayes classifier. Pay special attention to the `extract_features` function.\n",
        "\n",
        "2.**Train and Test**: Train the Naive Bayes classifier on the training data and test it on a separate test set. Evaluate the accuracy of the classifier.\n",
        "\n",
        "3.**Analysis (15 points)**: Discuss the results. Identify any misclassifications and try to understand why the classifier may fail in those cases. Provide examples of sentences that were not predicted correctly and explain possible reasons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M68XJubdKeDL",
        "outputId": "303a4574-ea4e-4e85-a39e-5e68417c1f14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "import math\n",
        "import string\n",
        "from collections import defaultdict\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import movie_reviews\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uV2WiST_qDit",
        "outputId": "d60e3f80-9e18-44ac-f352-2fd84d346ac5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "KSLo4_JoUcax"
      },
      "outputs": [],
      "source": [
        "def get_features(tokens):\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Perform stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "m2Whvjy_Jq8n"
      },
      "outputs": [],
      "source": [
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, classes):\n",
        "        self.classes = classes\n",
        "        self.class_probs = defaultdict(float)\n",
        "        self.class_counts = defaultdict(float)\n",
        "        self.feature_probs = defaultdict(lambda: defaultdict(float))\n",
        "        self.feature_counts = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    def train(self, training_data, alpha):\n",
        "        # Implement training here\n",
        "        # You should use get_features function to extract useful tokens from\n",
        "        # dataset and use them to train the classifier.\n",
        "        # Calculate class probabilities\n",
        "        total_docs = len(training_data)\n",
        "        for c in self.classes:\n",
        "            docs_in_class = sum(1 for doc, label in training_data if label == c)\n",
        "            self.class_probs[c] = docs_in_class / total_docs\n",
        "\n",
        "        # Calculate feature probabilities\n",
        "        for doc, label in training_data:\n",
        "            features = get_features(doc)\n",
        "            for feature in features:\n",
        "                self.feature_counts[feature][label] += 1\n",
        "\n",
        "        # Normalize feature probabilities and apply laplacian smoothing\n",
        "        for c in self.classes:\n",
        "            total_features_in_class = sum(self.feature_counts[feature][c] for feature in self.feature_counts)\n",
        "            for feature in self.feature_counts:\n",
        "                # We use log likelihood. instead of multiplying some probs, we add their logs to prevent vanishing of small numbers in coding.\n",
        "                self.feature_probs[feature][c] = math.log((self.feature_counts[feature][c] + alpha) / (total_features_in_class + alpha * len(self.feature_counts)))\n",
        "\n",
        "    def classify(self, features):\n",
        "      # Implement classification here\n",
        "      # Compute posterior probabilities\n",
        "      scores = {}\n",
        "      for c in self.classes:\n",
        "          score = math.log(self.class_probs[c]) # Probability of that class\n",
        "          for feature in features:\n",
        "              if feature in self.feature_probs:\n",
        "                  score += self.feature_probs[feature][c] # Probability of that token or word to appear in this class (sentiment)\n",
        "          scores[c] = score\n",
        "\n",
        "      # Predict the class with the highest score\n",
        "      return max(scores, key=scores.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "j2jeyI6nKooE"
      },
      "outputs": [],
      "source": [
        "# Load the movie reviews dataset from NLTK\n",
        "data = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "random.shuffle(data)\n",
        "\n",
        "# Shuffle the dataset for randomness\n",
        "random.shuffle(data)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(data) * split_ratio)\n",
        "train_set = data[:split_index]\n",
        "test_set = data[split_index:]\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "classes = set(sentiment for _, sentiment in train_set)\n",
        "classifier = NaiveBayesClassifier(classes)\n",
        "classifier.train(train_set, alpha = 1)\n",
        "\n",
        "def calculate_accuracy(dataset, dataset_type):\n",
        "    # Test the classifier on the testing set\n",
        "    correct_predictions = 0\n",
        "    for example in dataset:\n",
        "        tokens, true_sentiment = example\n",
        "        features = get_features(tokens)\n",
        "        predicted_sentiment = classifier.classify(features)\n",
        "        if predicted_sentiment == true_sentiment:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / len(dataset)\n",
        "    print(f\"{dataset_type} Accuracy: {accuracy}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNGQQR1wBPhQ",
        "outputId": "38622a54-fb5f-4914-e8d5-40103aea3394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.96875\n",
            "Test Accuracy: 0.79\n"
          ]
        }
      ],
      "source": [
        "calculate_accuracy(train_set, 'Train')\n",
        "calculate_accuracy(test_set, 'Test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqXiHKtYmD0l"
      },
      "source": [
        "### some examples of wrong predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sXDYhwEkpbU",
        "outputId": "72222264-c785-43bb-a920-cf8fdce609ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted: neg, ground_truth: pos, > from the commercials , this looks like a mild - mannered neil simonesque tale with mary tyler moore baring her bra touted as the highlight . instead it turns out to be a hilarious film running in high gear from beginning to end . the concept is deceptively pedestrian . an adult adopted son is looking for his biological parents and encounters eccentric characters along the way . the movie demonstrates just how far a good script and actors can take a mundane idea . the son and his wife take off on the search accompanied by a woman from the agency who located his parents . following one dead end lead after another , each funnier than the previous , they eventually end up in new mexico with his real biological parents : alan alda and lily tomlin . it ' s difficult to condense the mile - a - minute plot . seemingly hundreds of scenes jump on top of each other without giving you a chance to recover from the last one . without giving too much away , one of the better episodes involves a gay federal alcohol , tobacco and firearms agent attempting an arrest while tripping on lsd as his bi - sexual partner is upstairs licking the armpit of a woman while her husband is in the next room seducing their traveling companion . and it ' s all done in a fairly clean , almost ( well , maybe not exactly ) family fare manner . a grand cast ( tomlin , alda , moore , ben stiller , patricia arquette , tea leoni , george segal ) interacts in a seamless parade of laughs . drawing from a more hyper woody allen style , the film succeeds beyond expectations .\n",
            "predicted: pos, ground_truth: neg, \" marie couldn ' t talk , \" paulie , the parrot star of his own movie , tells us about the daughter in his original family . \" dad couldn ' t listen . and mom couldn ' t cope , so they got rid of me . \" paulie , the autobiography of a talking , not merely a mimicking , parrot , has jay mohr in the lead role of the bird - the voice , not the body - and as the minor character of benny , a small - time crook who uses paulie to pull off small scams like stealing twenties from atms . as the parrot , mohr is delightful when director john roberts allows him to cut up . benny , on the other hand , is a character you ' ve seen a thousand times before , and mohr brings nothing new to that role . roberts ' s deliberately slow pacing of laurie craig ' s script lends a subtle sweetness to its humor but creates some definite problems . when a kids ' movie wants to mosey along , watch out . if the material is not crisp and perfectly composed , beauty can sometimes dissolve into tedium . so it is with paulie . when they let their bird do his stand - up comedy routines , the show hums and the audience roars . too often , however , a sleepy silence ensues among the viewers as they wait for the story to pickup again . tony shalhoub , the smart - mouthed chef from big night , plays misha , a recent russian immigrant to the u . s . he had been a teacher of literature at home , but now he makes his living as a janitor in the animal research lab to which paulie has been taken for study . although misha gets a few nice lines ( \" i ' m russian . i like long stories . \" ) , his somber part seems designed only to elicit our sympathy . besides paulie , the only character worth noting - other than 2 cute small parts played by cheech marin and gina rowlands - is the speech - impaired marie , played in a precious performance by cinematic newcomer , 5 - year - old hallie kate eisenberg . naturally enchanting , she gives the picture genuine heart . the bad news is that her part is confined to the first half . the best scenes have the bird dancing and strutting to show off his comedic skills . when marie ' s family gets a cat , for example , the bird , who hasn ' t wanted to learn how to fly until then , takes an instant interest in soaring . tricking the cat while insulting him at the same time , paulie calls him a stupid hairball . their rapid physical antics add to the humor of the situation . it ' s good quality sitcom material but performed by animals . when one of the humans without much of a voice begins to sing , paulie cringes . \" i ' m a bird , \" he explains with his frequently subtle humor . \" i have a small brain , and it ' s about to explode . \" the movie contains rich doses of john debney ' s dreamy music . with heavy use of a solo violin , he keeps reinforcing the film ' s heart - warming themes . and when paulie finally takes off in flight , the orchestra comes up loud and strong with cymbals clashing . \" it ' s a long story , \" says paulie . \" it ' s the only kind he knows , \" reflects misha . and the motion picture , which runs the standard length for a kids ' movie , still feels too long . the best parts are enthralling , but then there are all of those dead spots in - between . paulie is a movie that never quite lives up to its promise but manages to charm nevertheless . paulie runs 1 : 32 . it is raged pg for a few mild profanities and would be fine for all ages . my son , jeffrey , age 9 , gave the movie * * with his biggest complaint being that there wasn ' t enough action . he thought paulie was funny , and the actress that played marie was quite good . his friend sam , almost 9 , thought the movie was \" awesome , excellent , \" and gave it * * * * . he thought paulie was good , but he didn ' t believe the way marie ' s speech impediment was acted .\n",
            "predicted: neg, ground_truth: pos, let ' s face it : since waterworld floated by , the summer movie season has grown * very * stale . with no new eye - candy for four weeks straight , we ' ve had to sustain ourselves on the quasi - nutritional value of cheatin ' husbands , traveling chocolate salesmen , and computer - generated serial killers . sigh . thank god for desperado . the freewheeling sequel to el mariachi -- director robert rodriguez ' s notorious $ 7000 debut -- stars a cool antonio banderas as the returning guitarist with no name . he ' s a man in black with revenge on his mind , and an arsenal in his case . ( the woman he loved was killed in the first film . ) so , he spends the entire story shooting drug dealers ; sort of a tex - mex version of the punisher , if you will . there isn ' t much of an emotional core to desperado . rodriguez is having too much fun finding new and innovative ways to pay homage to john woo . ( and sergio leone . . . and sam peckinpah . . . ) some may wince at the body count -- at least 100 graphic killings is a fair estimate -- but it ' s all played for laughs . big , broad , hispanic laughs that , for me , recall the physical comedy of blake edwards and his pink panther films . sick , slick fun .\n"
          ]
        }
      ],
      "source": [
        "for example in test_set[30:50]:\n",
        "  tokens, true_sentiment = example\n",
        "  features = get_features(tokens)\n",
        "  predicted_sentiment = classifier.classify(features)\n",
        "  if predicted_sentiment != true_sentiment:\n",
        "    text = \" \".join(tokens)\n",
        "    print(f\"predicted: {predicted_sentiment}, ground_truth: {true_sentiment}, {text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfl8UA42Gqjf"
      },
      "source": [
        "#Submission Instructions:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75kVTQX6GsCn"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "3.Clearly present the results of your parameter tuning in the notebook.\n",
        "\n",
        "4.Provide a brief summary of your findings and insights in the conclusion section.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Experiment with various seed texts to showcase the diversity of generated text.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
